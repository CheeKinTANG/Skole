\chapter{Architecture} \label{architecture}

The aim of this project was to investigate the possibility of creating hardware accelerator for a machine learning algorithm. We decided to create an architecture implementing \textit{the LeNet-5} \cite{LeCun1998} (see Figure \ref{fig_cnn}). The main reason for this was simply that the implementation would be easier. The input image is relatively small compared to the other architectures mentioned in Chapter \ref{chap_related_work}, which greatly reduces the amount of intermediate storage needed on chip. In addition the size of the kernels are the same in both convolution layers, making us able to reuse the convolution hardware accelerator for both layers. 

Sadly, creating a module that provides the full functionality of a CNN requires more time than what was available for this project. But we were able to realize part of the network, the forward pass of the convolution and the subsampling/pooling layer. For future references we will refer to our architecture as \textit{Imagezor}. This chapter will describe its design and the reasoning behind it. 


\section{Hardware}

In this project we had a Spartan 6 LX16 development board at our disposal, thus limiting our architecture to the hardware resources it provides. Since a convolution is basically a series of multiply and accumulate (MAC), \textit{digital signal processors} (DSP) are essential to accelerate the operation. The reason for this is that multiplication requires a lot of hardware resources on the FPGA, unless you uses a DSP, which is special-made-hardware for MAC operations. The Spartan 6 is equipped with 32 DSP slices. Ideally, we would have had several times as many DSPs in order to fully utilize the potential parallelism of the network. E.g. the architectures in \cite{Farabet2009} \cite{Farabet2010} \cite{Chakradhar2010} and \cite{Paper} used FPGA platforms with about 1000 DSPs. This will greatly limit the speed of our implementation, as it makes us unable to exploit intra- and inter parallelism (see Section \ref{sec_pot_parallelism}). But as we will see in laters sections, the 32 DSPs are enough to accelerate the convolution layer in the LeNet-5. 

\section{Imagezors Architecture}


Imagezor takes \textit{n} images as input, $ I_1, I_2, \dots, I_n $, a single kernel $ K $, and outputs a processed image $ O $.  Each image $ I_i $ is convolved with the kernel $ K $, and each respective pixel from each convolution is then summed, added a bias to, and sent through a non-linear function. Afterwards the resulting matrix is subsampled/max-pooled, where it is divided into $ p \times p $ non-overlapping neighborhoods, from which the maximum value is extracted. The result is the output image $ O $. Thus essentially Imagezor computes a single feature map based on the input images and a single kernel and performs a subsample/max-pool operation, and outputs the result. 

Using this architecture one can compute the convolution layer and subsample/pooling layer one feature map at a time. The workflow would go like this:

\begin{enumerate}
	\item Load the kernel associated with the respective feature map into the module.
	\item Load all the input images into a buffer the module can read from.
	\item Enable the module to start processing.
	\item Wait until 
\end{enumerate}


In order to support both of the layers, Imagezor takes in a control signal which determines which layer is currently is being processsed. 

The overall architecture of the convolution layer is separated into five modules:

\begin{itemize}
	\item \textbf{The convoluter}. Performs the convolution operation on the input.
	\item \textbf{The intermediate feature map buffer}. Since the resulting feature map is the sum of the convolutions of all the input images, this buffer is needed to store the results from the previous convolution, so that it can be added to the current convolution. In the first layer of the network there is only one input image (i.e. $ n = 1 $), thus no summation is needed.
	\item \textbf{Bias register}. Contains the bias value, and adds it to each convoluted pixel. 
	\item \textbf{Sigmoid}. Performs the non-linear sigmoid function  on the feature maps.
	\item \textbf{Subsample/max-pooler}. Performs the subsample/max-pool operation on the feature maps. 
\end{itemize}

\begin{figure}[h!]
  \centering
      \includegraphics[width=1.0\textwidth]{Figures/Method/conv_layer_arch}
  \caption{The architecture of Imagezor.}
\end{figure}


\subsection{The Convoluter} 

This module is inspired by \cite{Farabet2009}. The input is a $ n \times n $ image \textit{I}, and the output is a $ (n-k+1) \times (n-k+1) $ convolved image \textit{C}, using a $ k \times k $ kernel \textit{K}. Every clock cycle the module takes in a pixel as input, and after a certain delay it will output a processed pixel almost every cycle. Each pixel is inputted once, left to right, one row at a time. 

It consists of 2D grid of multiply and accumulate (MAC) units which represents the convolution kernel. Thus the grid dimension is equal to the kernel dimension. In every MAC unit there is a register that contains the respective kernel weight. In every clock cycle the MAC units multiply the input pixel with its weight, and then accumulates the result from the previous cycle of the MAC unit to the left. 

At the end of each row of MACs there is $ n - k $ shift registers. The result of the last MAC in each row is stored in the first shift register, and the first MAC in each row takes the value of the last shift register of the previous row as accumulation input. The exception being the absolute first and last MAC unit. Every clock cycle the values in the shift registers are shifted to the right. 

\begin{figure}[h!]
  \centering
      \includegraphics[width=0.7\textwidth]{Figures/Method/Convolver}
  \caption{The convolution module, when $ n = 3 $ and $ k = 2 $.}
\end{figure}
	
By providing this delay you only have to input each pixel once during the convolution. Generally every pixel is needed for $ k \times k $ convolution operations (the exception being the pixels close to the boarders of the image). Thus the shift registers are used to store the intermediate values of the convolutions until a pixel that is needed for the respective convolution operation is inputted. 

The delay these shift registers cause are the reason for the delay before valid output pixels are produced. Thus from when the convolution starts, the output will not be valid before $ k-1 $ rows of the image have been processed. And for every new image row, there will be a $ k-1 $ cycle delay before output is valid. This is demonstrated by the fact that the input image is a $ n \times n $ matrix, while the output matrix is a $ (n-k+1) \times (n-k+1) $ matrix. 

The loading of the weights takes $ k \times k $ clock cycles, and the processing of the image takes $ n \times n $ clock cycles. Thus the total number of cycles it takes to perform a full convolution of an image is $ n \times n + k \times k $. But based upon the papers refered to in Section \ref{sec_related_work} it seems that \textit{n} tends to be larger than \textit{k}. E.g. for the first layer in the LeNet-5 \cite{LeCun1998}, $ n = 32 $ and $ k = 5 $, the loading  of the weights take 25 clock cycles and the image processing 1024 cycles. This means that the execution time of the Convoluter is primairly bounded by the size of the image. But the size of the kernel decides the hardware resource cost of the module, since it requires $ k \times k $ DSP slices on the FPGA.

\begin{figure}[h!]
  \centering
      \includegraphics[width=1.1\textwidth]{Figures/Method/Conv_example}
  \caption{Example showing the five first clock cycle of an convolution. The weights of the kernel is already loaded into the MAC units, and every cycle a new pixel from the image in inputted. In the last example you can see that 42 is provided as the first output.}
\end{figure}

\paragraph{The Intermediate image buffer}

\paragraph{The Sigmoid} module is based upon \cite{Napocensis2009} using PLAN approximation. It takes input a single value \textit{x} and outputs an approximation of the sigmoid function.

\vspace*{1\baselineskip}
REWRITE FIRST PART
\subsection{The Max Pooler} 

The Max Pooler (SS/MP) was designed to complement the CM and avoid being a bottleneck. Thus it was designed to act in the same streaming way as the convolution module, and be done processing at almost the same time. The input is a $ (n-k+1) \times (n-k+1) $ convolved image \textit{C}, and the output is a $ (n-k+1)/p \times (n-k+1)/p $ SS/MP image \textit{P}, where \textit{p} is the dimension of subsample neighborhood. As with the CM, one pixel is streamed in every cycle, and streamed out whenever a valid pixel is ready. Designed this way the SS/MP can process in parallel with the CM, by directly streaming the output of the CM into the SS/MP module. 

\begin{figure}[h!]
  \centering
      \includegraphics[width=0.5\textwidth]{Figures/Method/submax}
  \caption{Architecture of the subsampler/max-pooler.}
\end{figure}

The module compares the input with the current max value, and updates the max value accordingly. It contains a set of \textit{(n-k+1)/p} shift registers. Since the image is divided into $ p \times p $ non-overlapping neighborhoods, the module needs to store the current maximum value of previous neighborhood when a pixel from a new neighborhood is inputted. To do this the module contains two counters, \textit{row\_num} and \textit{column\_num}. When a new pixel is inputted the \textit{column\_num} counter is incremented, and when a new row is encountered the \textit{row\_num} counter is incremented. Every time $ column_num mod p = 0 $ the shift registers are shifted one to the right, and every time $ column_num mod p = 0 and row_num mod p = 0 $ a valid output is produced. 

The execution speed of the SS/MP module is bounded by the size of the input image, $ c \times c $ clock cycles, finishing one cycle after the last pixel has been inputted. 
Thus by streaming the output of the CM to the SS/MP, both will finish only a few cycles apart, effectively running both jobs in parallel. The resource usage of the module is bounded by the size of the subsampling dimension, since it requires a number of shift registers equal to the size of the dimension. But essentially its resource usage is quite low.  
