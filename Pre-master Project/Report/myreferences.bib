@book{Bishop2006,
author = {Bishop, Christopher M.},
publisher = {Springer Science+Buisness Media, LLC},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{Chakradhar2010,
address = {New York, New York, USA},
author = {Chakradhar, Srimat and Sankaradas, Murugan and Jakkula, Venkata and Cadambi, Srihari},
doi = {10.1145/1815961.1815993},
file = {:home/magnus/Github/Skole/Pre-master Project/References/ADynamicConfigCoprocessorForCNN.pdf:pdf},
isbn = {9781450300537},
journal = {Proceedings of the 37th annual international symposium on Computer architecture - ISCA '10},
pages = {247},
publisher = {ACM Press},
title = {{A dynamically configurable coprocessor for convolutional neural networks}},
url = {http://portal.acm.org/citation.cfm?doid=1815961.1815993},
year = {2010}
}
@article{Cires2003,
author = {Cires, Dan C and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M},
file = {:home/magnus/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cires et al. - 2011 - Flexible , High Performance Convolutional Neural Networks for Image Classification.pdf:pdf},
journal = {IJCAI'11 Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Volume Two},
pages = {1237--1242},
title = {{Flexible , High Performance Convolutional Neural Networks for Image Classification}},
year = {2011}
}
@article{Ciresan2012,
abstract = {We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46\%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.},
author = {Cire≈üan, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J\"{u}rgen},
doi = {10.1016/j.neunet.2012.02.023},
file = {:home/magnus/Github/Skole/Pre-master Project/References/trafficContestGpu.pdf:pdf},
issn = {1879-2782},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Algorithms,Automatic Data Processing,Automobile Driving,Automobile Driving: psychology,Computer Graphics,Motor Vehicles,Neural Networks (Computer),Pattern Recognition, Automated,Vision, Ocular,Vision, Ocular: physiology},
month = aug,
pages = {333--8},
pmid = {22386783},
title = {{Multi-column deep neural network for traffic sign classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22386783},
volume = {32},
year = {2012}
}
@article{Farabet2010,
author = {Farabet, Clement and Martini, Berin and Akselrod, Polina and Talay, Selcuk and LeCun, Yann and Culurciello, Eugenio},
doi = {10.1109/ISCAS.2010.5537908},
file = {:home/magnus/Github/Skole/Pre-master Project/References/largeScaleCNNFPGA.pdf:pdf},
isbn = {978-1-4244-5308-5},
journal = {Proceedings of 2010 IEEE International Symposium on Circuits and Systems},
month = may,
pages = {257--260},
publisher = {Ieee},
title = {{Hardware accelerated convolutional neural networks for synthetic vision systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5537908},
year = {2010}
}
@article{Farabet2009,
abstract = {Convolutional networks (ConvNets) are biologically inspired hierarchical architectures that can be trained to perform a variety of detection, recognition and segmentation tasks. ConvNets have a feed-forward architecture consisting of multiple linear convolution filters interspersed with pointwise non-linear squashing functions. This paper presents an efficient implementation of ConvNets on a low-end DSP-oriented field programmable gate array (FPGA). The implementation exploits the inherent parallelism of ConvNets and takes full advantage of multiple hardware multiply accumulate units on the FPGA. The entire system uses a single FPGA with an external memory module, and no extra parts. A network compiler software was implemented, which takes a description of a trained ConvNet and compiles it into a sequence of instructions for the ConvNet Processor (CNP). A ConvNet face detection system was implemented and tested. Face detection on a 512 times 384 frame takes 100 ms (10 frames per second), which corresponds to an average performance of 3.4 times 10<sup>9</sup> connections per second for this 340 million connection network. The design can be used for low-power, lightweight embedded vision systems for micro-UAVs and other small robots.},
author = {Farabet, Cl\'{e}ment and Poulet, Cyril and Han, Jefferson Y. and LeCun, Yann},
file = {:home/magnus/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farabet et al. - 2009 - CNP An FPGA-based processor for Convolutional Networks.pdf:pdf},
isbn = {9781424438921},
journal = {FPL 09: 19th International Conference on Field Programmable Logic and Applications},
number = {1},
pages = {32--37},
title = {{CNP: An FPGA-based processor for Convolutional Networks}},
volume = {1},
year = {2009}
}
@article{Fukushima1980,
author = {Fukushima, Kunihiko},
file = {:home/magnus/Github/Skole/Pre-master Project/References/Fukushima1980.pdf:pdf},
journal = {Biol. Cybernetics},
pages = {193--202},
title = {{Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position}},
volume = {202},
year = {1980}
}
@article{Fukushima1982,
author = {Fukushima, Kunihiko and Miyake, Sei},
doi = {10.1016/0031-3203(82)90024-3},
file = {:home/magnus/Github/Skole/Pre-master Project/References/Fukushima\_Miyake1981.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
month = jan,
number = {6},
pages = {455--469},
title = {{Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0031320382900243},
volume = {15},
year = {1982}
}
@article{Paper,
author = {Gokhale, Vinayak and Jin, Jognhoon and Dundar, Aysegul and Martini, Berin and Culurciello, Eugenio},
file = {:home/magnus/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gokhale et al. - 2014 - A 240 G-ops s Mobile Coprocessor for Deep Neural Networks.pdf:pdf},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
pages = {682--687},
title = {{A 240 G-ops / s Mobile Coprocessor for Deep Neural Networks}},
year = {2014}
}
@article{Jarrett2009,
abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63\% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6\%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\&amp;gt; 65\%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53\%).},
author = {Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
file = {:home/magnus/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jarrett et al. - 2009 - What is the best multi-stage architecture for object recognition.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2146--2153},
title = {{What is the best multi-stage architecture for object recognition?}},
year = {2009}
}
@book{Jordan2006,
abstract = {Methods of dimensionality reduction provide a way to understand and visualize the structure of complex data sets. Traditional methods like principal component analysis and classical metric multidimensional scaling suffer from being based on linear models. Until recently, very few methods were able to reduce the data dimensionality in a nonlinear way. However, since the late nineties, many new methods have been developed and nonlinear dimensionality reduction, also called manifold learning, has become a hot topic. New advances that account for this rapid growth are, e.g. the use of graphs to represent the manifold topology, and the use of new metrics like the geodesic distance. In addition, new optimization schemes, based on kernel techniques and spectral decomposition, have lead to spectral embedding, which encompasses many of the recently developed methods. This book describes existing and advanced methods to reduce the dimensionality of numerical databases. For each method, the description starts from intuitive ideas, develops the necessary mathematical details, and ends by outlining the algorithmic implementation. Methods are compared with each other with the help of different illustrative examples. The purpose of the book is to summarize clear facts and ideas about well-known methods as well as recent developments in the topic of nonlinear dimensionality reduction. With this goal in mind, methods are all described from a unifying point of view, in order to highlight their respective strengths and shortcomings. The book is primarily intended for statisticians, computer scientists and data analysts. It is also accessible to other practitioners having a basic background in statistics and/or computational learning, like psychologists (in psychometry) and economists.},
author = {Jordan, M and Kleinberg, J},
booktitle = {Pattern Recognition},
editor = {Jordan, Michael and Kleinberg, Jon M and Scholkopf, Bernhard},
file = {:home/magnus/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jordan, Kleinberg - 2006 - Information Science and Statistics.pdf:pdf},
isbn = {9780387310732},
number = {356},
pages = {791--799},
publisher = {Springer New York},
title = {{Information Science and Statistics}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Le2011,
author = {Le, Quoc V and Coates, Adam and Prochnow, Bobby and Ng, Andrew Y},
file = {:home/magnus/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le et al. - 2011 - On Optimization Methods for Deep Learning.pdf:pdf},
title = {{On Optimization Methods for Deep Learning}},
year = {2011}
}
@article{LeCun1998,
author = {LeCun, Yann and Bottou, L\'{e}on and Bengio, Yoshua and Haffner, Patrick.},
file = {:home/magnus/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
title = {{Gradient-Based Learning Applied to Document Recognition}},
year = {1998}
}
@article{Leonard1990,
author = {Leonard, J. and Kramer, M. A.},
file = {:home/magnus/Github/Skole/Pre-master Project/References/backprop\_new.pdf:pdf},
journal = {Computers \& Chemical Engineering},
number = {3},
pages = {337--341},
title = {{Improvement of the backpropagation algorithm for training neural networks}},
volume = {14},
year = {1990}
}
@book{Minsky1969,
author = {Minsky, Marvin and Papert, Seymour},
title = {{Perceptrons: an introduction to computational geometry}},
year = {1969}
}
@article{Napocensis2009,
author = {Tisan, A and Oniga, S and MIC, D and Buchman, A},
file = {:home/magnus/Github/Skole/Pre-master Project/References/SigmoidApproximation.pdf:pdf},
journal = {ACTA TECHNICA \ldots},
keywords = {fpga implementation,sigmoid approximation,system generator},
number = {2},
pages = {15--20},
title = {{Digital Implementation of The Sigmoid Function for FPGA Circuits}},
url = {http://users.utcluj.ro/~atn/papers/ATN\_2\_2009\_4.pdf},
volume = {50},
year = {2009}
}
@article{Rumelhart1986,
author = {Rumelhart, DE and Hinton, GE and Williams, RJ},
file = {:home/magnus/Github/Skole/Pre-master Project/References/backprop\_old.pdf:pdf},
journal = {Nature},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
url = {http://books.google.com/books?hl=en\&lr=\&id=FJblV\_iOPjIC\&oi=fnd\&pg=PA213\&dq=Learning+representations+by+back-propagating+errors\&ots=zZDq4iF\_WT\&sig=FdAeaFWySS4z4Li9WdNPqSB4HdA},
volume = {323},
year = {1986}
}
@article{Sankaradas2009,
author = {Sankaradas, Murugan and Jakkula, Venkata and Cadambi, Srihari and Chakradhar, Srimat and Durdanovic, Igor and Cosatto, Eric and Graf, Hans Peter},
doi = {10.1109/ASAP.2009.25},
file = {:home/magnus/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sankaradas et al. - 2009 - A Massively Parallel Coprocessor for Convolutional Neural Networks.pdf:pdf},
journal = {2009 20th IEEE International Conference on Application-specific Systems, Architectures and Processors},
month = jul,
pages = {53--60},
publisher = {Ieee},
title = {{A Massively Parallel Coprocessor for Convolutional Neural Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5200010},
year = {2009}
}
@article{Simard2000,
author = {Simard, PY and Steinkraus, Dave and Platt, JC},
file = {:home/magnus/Github/Skole/Pre-master Project/References/bestPracticeCNN.pdf:pdf},
journal = {2013 12th International \ldots},
title = {{Best practices for convolutional neural networks applied to visual document analysis}},
url = {http://www.computer.org/csdl/proceedings/icdar/2003/1960/02/196020958.pdf},
year = {2003}
}
