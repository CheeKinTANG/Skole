\chapter{Related Work}

This section will give an overview of the current state of research on Artificial Neural Networks and Convolutional Neural Networks. 

\section{Neural networks}
The mathematical fundamentals for Convolutional Neural Networks was introduced as early as in the 1980s by Kunihiko Fukushima\cite{Fukushima1980}\cite{Fukushima1982}, in form of the \textit{neocognitron} model. The model was later improved in 1998 by  Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner - who introduced the \textit{Convolutional Neural Network} model. In 2003 the model was simplified by Patrice Simard, David Steinkraus, and John C. Platt \cite{Simard2000}, in an attempt to make it easier to implement. The paper also mentions two of the main issues with CNNs, size of the training set and time spent training. In order to achieve high enough accuracy a CNN requires thousands of training samples, which needs to be labeled. Processing all of these samples and fine-tuning the networks takes a great amount of processing power, causing training to take days or weeks. These issues are the ones that caused CNNs not to gain popularity before mid-2000. The rise of the Internet, digital cameras, and Big Data have provided us with vast amounts of images which can be used for training, and improvement in the speed and sophistication of computer hardware have reduced the training time from days/weeks to hours. E.g. \cite{Cires2003} purposes a GPU implementations which reduced the epoch (see section \ref{ann}) training time from 35 hours to 35 minutes. 

These recent advancements have renewed the interest in neural networks, and increased the research done on the field. As a result CNNs have become a leading model within pattern recognition for computer vision. This can be illustrated by the fact that CNNs implementations have won several pattern recognition contests in the period 2009-2012, including IJCNN 2011 Traffic Sign Recognition Competition\cite{Ciresan2012} and the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge\cite{DanC.Ciresan2012}.


\section{Neural networks in hardware}

In \cite{Farabet2009} a CNN was implemented on a Virtex-4 SX35 FPGA from Xilinx. In this implementation all the fundamental computations was hard-wired, and controlled by a 32bit soft processor using macro-instructions. Training was done offline, and a representation of the network was provided to the soft processor. With this implementation they were able to process a $ 512 \times 384 $ grayscale image in 100\textit{ms}, i.e. 10 frames per second. 