\chapter{Introduction}

\section{Motivation}

A \textit{Convolutional Neural Network} (CNN) is a deep-learning algorithm architecture that has become increasingly popular in the last decade. It is considered a state of art technique for object recognition in images and sound, and it is applied in application such as video surveillance, mobile robot vision, image search in data centers, and more \cite{Zhang2015} \cite{Farabet2009} \cite{Ji2013} \cite{Ovtcharov2015}. With the Internet-of-Things and today's tremendous amount of devices able to capture pictures and videos, the potential for CNNs have vastly increased. By making our devices able to recognize its surroundings, there could be a numerous amount of potential interesting applications.

Albeit CNNs perform great in terms of accuracy (see Chapter \ref{chap_related_work}), they are very computational heavy, which have limited their usability until recent years. The computational structure of neural networks is highly parallelizable, which, when exploited, can greatly increase performance. It is for this reason that \textit{general-purpose central processing units} (GPCPUs) performs poorly when computing such networks, as they are primarily designed for effective serial computations, and are thus unable to exploit its parallel structure. \textit{Field-programmable gate arrays} (FPGAs), \textit{graphic processor units} (GPUs) and \textit{application-specific integrated circuits} (ASICs) are hardware components that are (or can be) built to heavily exploit parallelism, and have been shown to greatly outperform CPUs in parallel applications \cite{Chung2010}.

While GPUs performs incredibly well on parallel applications, they have a major drawback: power consumption. With power being the primary financial cost of data centers and mobile devices having to operate on a limited power budget \cite{Duranton2013}, GPUs can be unsuitable for several applications. Thus, for CNN applications that require lower power and high performance, FPGA and ASIC accelerators have become increasingly popular (see Section \ref{sec_related_work_cnn}). Simplistically, an ASIC is an processor that is made specifically for one application,  while a FPGA is a component that contains a set of programmable logic blocks that can be configured to have the same behavior as any arbitrary circuit. I.e. a FPGA is a reconfigurable ASIC. 

In our previous work \cite{Halvorsen2014}, we investigated the mathematical model behind neural networks and purposed a unimplemented accelerator architecture. In this project, we have implemented a tweaked version of this architecture on a Zynq FPGA, and constructed all the supporting hardware and software components needed to get it running. In order to test its capabilities, we have used it to compute the LeNet-5 \cite{LeCun1998}, which recognizes handwritten digits. Our system has shown to be 5.6x times as fast and power efficient than an ARM Cortex-A9 processor, and 5x as power efficient as an Intel Core i7 4710HQ CPU on certain parts of the processing.


\section{Assignment Interpretation}

Based on the assignment description text, the following main tasks were
identified:\\ \hfil \\ \hfil
\textbf{Task 1 \textit{(mandatory)}} Implement a hardware accelerator for a Convolutional Neural Network, with the intention of improving energy efficiency.  \\ \hfil \\ \hfil
\textbf{Task 2 \textit{(mandatory)}} Compare our accelerator to an equivalent pure-software implementation on a general-purpose CPU, primarily in terms of power consumption.  \\ \hfil \\ \hfil
\textbf{Task 3 \textit{(optional)}} Implement said system on a  Zynq FPGA board, but weigh the advantages and disadvantages of other platforms, such as SHMAC or other FPGA platforms. \\ \hfil \\ \hfil
\textbf{Task 4 \textit{(optional)}} Extend the system to be able to recognize objects from a web-cam stream. \\ \hfil \\ \hfil


\section{Report structure}

For the convenience of the reader, we will here provide a quick overview of the topic of the report's chapters. \\ \hfil \\ \hfil
\textbf{Chapter 2: Background} gives an introduction to the mathematical model of Artificial Neural Networks and Convolutional Neural Networks. \\ \hfil \\ \hfil
\textbf{Chapter 3: Related Work} gives an overview of the state of the art CNNs and the most relevant recent hardware implementations. \\ \hfil \\ \hfil
\textbf{Chapter 4: Architecture} presents our implemented design for a CNN hardware accelerator.  \\ \hfil \\ \hfil
\textbf{Chapter 5: Results and Discussion} compares our design with an equivalent implementation on a ARM and laptop CPU, in terms of performance and energy efficiency, and the hardware resource usage of our design. The chapter will also provide our analysis of the results.  \\ \hfil \\ \hfil
\textbf{Chapter 6: Future Work} presents how our design can be further improved. \\ \hfil \\ \hfil
\textbf{Chapter 7: Conclusion} provides concluding remarks and a summary of the identified tasks.\\ \hfil \\ \hfil



