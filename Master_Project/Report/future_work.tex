\chapter{Future work} \label{chap_future_work}

Developing a convolutional neural network hardware accelerator is a complex and time consuming task. There are thus several performance related improvements that we would wish to have implemented and tested, but sadly we ran out of time. In this chapter we will give an overview the planned, but unfinished, features we would wish to extend to our current architecture. The features are listed in a priority order, and we will give some indication of how much work is required to implement said features.


\begin{enumerate}                           


	\item \textbf{Acceleration of layer C5.}
	
-	Will probably try this before delivering. 
%	While improvements can still be made to imrove the current accelerated layers, the relative gain would be so little compared to accelerating C5, and potentially F6. 
	
	\item \textbf{Explore ways to reduce the resource usage.}
	
	The primary focus of this project has been to get the prototype up and running, and little thought have gone to examine ways to minimize resource consumption. Currently we only have enough resources to fit two accelerators on the board. Thus if we wish to extend the design and/or run several accelerators in parallel, we either need to change to a bigger board or minimize the resource usage of our design. Any future work would do well to explore this area. 
	

	
	\item \textbf{More accelerators in parallel.}
	
	Currently we are only running two accelerators in parallel, which both have a respective DMA that moves their input and output data. The maximum DDR bandwidth is at about 3.2 GB/s, and each DMA has access to a \textit{high-performance} port which can  deliver up to 800 MB/s. This effectively means that we are exploiting half of the available DDR bandwidth. Given a big enough hardware platform or improvements to resource usage, as mentioned in the above point, this feature should be simple to implement. 
	
	\item \textbf{Stay in hardware, instead of going back to software for next layer}
	
	The main reason \cite{Zhang2015} and \cite{Ovtcharov2015} achieved high performance was by reducing off-chip traffic. In the current state of our system, software has to be involved for every feature map to be computed, and data is being transfered back and forth between software and hardware several times. This is inefficient. Thus extending the system with logic that can redistrubute the output maps as new input maps without involving software could provide a substantial performance boost. But it will increase resource usage and development time, and will probably require a bigger board. Both the mentioned papers used FPGAs at the size of a Virtex 7.  
	
	\item \textbf{Stream data through accelerator, instead of filling the buffer first.}
	
	Currently all the data required for computations are loaded into a buffer before it is processed by the accelerator. Changing it so that the data can be streamed directly into the accelerator would provide two potential benefits: 1) faster processing, since data can be processed while the DMA is transfering data to the accelerator, 2) reduced storage on chip, since we no longer need to store all the data in an input buffer. Should be fairly easy to extend the design for this, but due to time constraints, and the gains probably not being that great, it was not prioritized. 
	
	\item \textbf{Hardware accelerate float to fixed.}
	
	Currently our system pre-processes the image and weights into fixed point before processing them. If this system were to be integrated into system using floating points it would be beneficial to do this transformation in hardware. Currently we are able to do fixed point to floating point in hardware using only one clock cycle, and thus we believe it should be possible to do the same for float to fixed. 

  \item Test with bigger images.

    \item Better memory scheme. Keep all input maps in hardware buffer. 
    
	\item \textbf{Training on hardware.}
	
\end{enumerate}
