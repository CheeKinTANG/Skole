\chapter{Related Work} \label{chap_related_work}

This section will give an overview of the current state of research on Convolutional Neural Networks. 

\section{Convolutional Neural Networks}
The mathematical fundamentals for Convolutional Neural Networks was introduced as early as in the 1980s by Kunihiko Fukushima\cite{Fukushima1980}\cite{Fukushima1982}, in form of the \textit{neocognitron} model. The model was later improved in 1998 by  Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner - who introduced the \textit{Convolutional Neural Network} model. In 2003 the model was simplified by Patrice Simard, David Steinkraus, and John C. Platt \cite{Simard2000}, in an attempt to make it easier to implement. The paper also mentions two of the main issues with CNNs: the size of the training set and the time spent training. In order to achieve high enough accuracy a CNN requires thousands of training samples, which needs to be labeled. Processing all of these samples and fine-tuning the networks takes a great amount of processing power, causing training to take days or weeks. These issues are the ones that caused CNNs not to gain popularity before mid-2000. The rise of the Internet, digital cameras, and Big Data have provided us with vast amounts of images which can be used for training. Improvements in the speed and sophistication of computer hardware have reduced the training time from days/weeks to hours. E.g. \cite{Cires2003} purposes a GPU implementations which reduced the epoch (see Section \ref{sec_ann_training}) training time from 35 hours to 35 minutes. This demonstrates that highly parallel hardware vastly increases the effiency of neural networks compared to CPUs. 

These recent advancements have renewed the interest in neural networks, and increased the research done on the field. As a result CNNs have become a leading model within pattern recognition for computer vision. This can be illustrated by the fact that CNNs implementations have won several pattern recognition contests in the period 2009-2012, including IJCNN 2011 Traffic Sign Recognition Competition\cite{Ciresan2012} and the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge\cite{DanC.Ciresan2012}.


\section{Convolutional Neural Network in Hardware} \label{sec_related_work_cnn}

There have been several proposed hardware architectures during the last decade, and below we will describe the more recent and relevant architectures. If the reader is interested in even older implementations, one can refer to \cite{Girones1999}, \cite{Benkrid2002}, \cite{Cardells-Tormo2005}, \cite{Zhang2007}, and \cite{Savich2007}. We have divided the presented the architectures into two categories, mobile co-processors and server co-processors. The first is small architectures that are intended to fit into resource constrained environments, i.e. mobile applications, while the second is larger architectures that have virtually no resource constraint. But a common design goal for both categories are power efficiency. 

\paragraph{Mobile co-processors} \hfil \\
In \cite{Farabet2009} a CNN was implemented on a Virtex-4 SX35 FPGA from Xilinx. In this implementation all the fundamental operations were accelerated by a special-made ALU, and controlled by a 32~bit soft processor using macro instructions. That is, they created macro instructions for convolution, non-linear function, subsampling/pool and dot product between values at identical locations in multiple 2D planes and a vector. Training was done offline, and a representation of the network was provided to the soft processor. With this implementation they were able to process a $ 512 \times 384 $ gray-scale image in 100\textit{ms}, i.e. 10 frames per second. The design was intended for use in low power embedded vision  systems, e.g. robots, and the whole circuit board used less than 15 W.

Farabet and LeCun later improved the mentioned architecture in \cite{Farabet2010}. In this design they added multiple parallel vector processing units and allowed individual
streams of data to operate within processing blocks. They were able to achieve 30 frames per second using 15 W. In addition they predicted a planned ASIC implementation of the system would increase the processing speed and reduce the power to 1 W. 

In \cite{Chakradhar2010} they explore how they can exploit  the parallel nature of CNNs. They introduce two types of parallelism found in CNNs, \textit{inter-output} and \textit{intra-output}. The first one comes from the observation that each feature map and the corresponding subsampling/pooling computation can be done in parallel. This is easily seen in the first layer. The second one refers to that the convolution of several inputs are combined to produce one feature map (see Figure \ref{fig_visual_conv_ss_mp}), where the individual convolutions can be done in parallel. This one is present in all of the convolution layers after the first layer. They exploit these observations by purposing a dynamically configurable co-processor on a FPGA, which can switch between computing several different feature maps in parallel and processing several inputs to compute one feature map. By doing this they are able to fully utilize the parallel nature of a CNN and reduce the intermediate storage on the FPGA. Using a Virtex 5 SX240T FPGA with 1024 multiply-accumulate units they were able to outperform  a 2.3 GHz quad-core, dual socket Intel Xeon, and a 1.35 GHz C870 GPU by 4x to 8x. 

\cite{Paper} presents an architecture they named the \textit{nn-X}. For the implementation they used a Xilinx ZC706 platform, containing a Kintex-7 FPGA and two ARM Cortex-A9. They made a set of \textit{collections} that contained acceleration units for the convolution and subsampling/pooling operations. Each collection also contained a data router which could route data to the accelerator units, or to other collections in order to share data. The convolution and subsample/pooling layer was procssed on the FPGA using the accelerators, while the fully connected layer was processed by the arm processors. The authors claim that this architecture is the fastest and most power efficient of all the purposed architectures for mobile processors, to date. It is able to perform up to 227 G-ops/s, using 8W.  

\cite{Dundar2014} focuses one the challenges of memory bandwidth related to deep convolutional neural networks. They argue that while accelerators are fast, slow memory makes it difficult to saturate the accelerators with enough data. To combat this they purpose a memory access optimized routing scheme, where they try do reduce the number of times a input map has to be transfered from memory to the accelerator. A crucial point here is that in general the output map is the sum of several convoluted input maps. Thus if a accelerator is only able to compute one output map at a time, the input maps have to be transfered to the accelerator several times. This architecture suggested reduces the amount of such transfers by having a DMA for every two accelerator, and making the DMA transfer the same data to both of its accelerators. The accelerators will either produce an intermediate results or a complete output map, depending on how many iterations it has run. There is a total if eight accelerators, four which used to combine intermediate results into complete output maps, and four to compute intermediate results. Using this memory scheme they were able to decrease the memory access by 2x and increase the hardware utilization by 2x. 

\paragraph{Server co-processors} \hfil \\
Hardware acceleration of CNNs have also gained popularity within the data center field. The Internet and Big Data have made it viable to have servers that performs image classification, image recognition and natural language processing, using CNNs. Since the main expense of data centers are power usage, using specialized hardware accelerators that provides good performance at low power have gained increased popularity. Thus recently there have appeared architecture suggestions for much bigger FPGAs than the previously mentioned, since size is mostly a problem for mobile applications. 

One of the most prominent architectures is the one suggested in \cite{Zhang2015}. In this paper they present a detailed analysis of computing throughput and memory bandwidth utilization. Using the roofline model \cite{Williams2009} they explored the design space in order to detect possible optimizations, including loop tiling, unrolling and pipelining. Based on these analysis they purpose a architecture for an hardware accelerator. What separates this architecture from the previous ones is mainly that the accelerator computes the whole layer in one go, instead of parts of the layer and combining them later. That is, all the input data of a layer is inputted to the accelerator, it computes, and outputs all of the output feature maps of that layer. Previous implementations have primarily accelerated parts of the layer, or one feature map at a time. This greatly decrease the off-chip traffic, which is said to be the main performance sink for CNN accelerators. Such an architecture requires an extensive amount of hardware resources, which is why they implemented it on a Virtex 7. The reward is a throughput of 61.62 GFLOPS, using 18.6 W.   

Microsoft, who have experimented on using accelerators for CNNs in their data centers, recently purposed an architecture which exceeds the previous one. In \cite{Ovtcharov2015} they present an hardware accelerator that fit into a Stratix V D5 FPGA, and that can be integrated in their severs. Again, the main optimization is preventing off-chip memory, which they achieve by using an on-chip data re-distributor, making them able to compute several layers in a row. While the previous mentioned architecture computed one layer at a time, this architecture computes several, boosting the performance by 3x compared to the previous. Thus the system is still slower than a GPU implementation on a Tesla K40, which is 6x faster, but the accelerator is at least 2x as energy efficient.
 

