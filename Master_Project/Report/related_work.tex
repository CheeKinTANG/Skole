\chapter{Related Work} \label{chap_related_work}

This section will give an overview of the current state of research on Convolutional Neural Networks. 

\section{Convolutional Neural Networks}
The mathematical fundamentals for Convolutional Neural Networks was introduced as early as in the 1980s by Kunihiko Fukushima\cite{Fukushima1980}\cite{Fukushima1982}, in form of the \textit{neocognitron} model. The model was later improved in 1998 by  Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner - who introduced the \textit{Convolutional Neural Network} model. In 2003 the model was simplified by Patrice Simard, David Steinkraus, and John C. Platt \cite{Simard2000}, in an attempt to make it easier to implement. The paper also mentions two of the main issues with CNNs: the size of the training set and the time spent training. In order to achieve high enough accuracy a CNN requires thousands of training samples, which needs to be labeled. Processing all of these samples and fine-tuning the networks takes a great amount of processing power, causing training to take days or weeks. These issues are the ones that caused CNNs not to gain popularity before mid-2000. The rise of the Internet, digital cameras, and Big Data have provided us with vast amounts of images which can be used for training. Improvements in the speed and sophistication of computer hardware have reduced the training time from days/weeks to hours. E.g. \cite{Cires2003} purposes a GPU implementations which reduced the epoch (see Section \ref{sec_ann_training}) training time from 35 hours to 35 minutes. This demonstrates that highly parallel hardware vastly increases the effiency of neural networks compared to CPUs. 

These recent advancements have renewed the interest in neural networks, and increased the research done on the field. As a result CNNs have become a leading model within pattern recognition for computer vision. This can be illustrated by the fact that CNNs implementations have won several pattern recognition contests in the period 2009-2012, including IJCNN 2011 Traffic Sign Recognition Competition\cite{Ciresan2012} and the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge\cite{DanC.Ciresan2012}.


\section{Convolutional Neural Network in Hardware} \label{sec_related_work_cnn}

There have been several proposed hardware architectures during the last decade, and below we will describe the more recent and relevant architectures. If the reader is interested in even older implementations, one can refer to \cite{Benkrid2002}  \cite{Cardells-Tormo2005} \cite{Hui2007} \cite{Savich2007} \cite{Girones2005}.

In \cite{Farabet2009} a CNN was implemented on a Virtex-4 SX35 FPGA from Xilinx. In this implementation all the fundamental operations were accelerated by a special-made ALU, and controlled by a 32~bit soft processor using macro instructions. That is, they created macro instructions for convolution, non-linear function, subsampling/pool and dot product between values at identical locations in multiple 2D planes and a vector. Training was done offline, and a representation of the network was provided to the soft processor. With this implementation they were able to process a $ 512 \times 384 $ grayscale image in 100\textit{ms}, i.e. 10 frames per second. The design was intented for use in low power embedded vision  systems, e.g. robots, and the whole curcuit board used less than 15 W.

Farabet and LeCun later improved the mentioned architecture in \cite{Farabet2010}. In this design they added multiple parallel vector processing units and allowed individual
streams of data to operate within processing blocks. They were able to achieve 30 frames per second using 15 W. In addition they predicted a planned ASIC implementation of the system would increase the processing speed and reduce the power to 1 W. 


In \cite{Chakradhar2010} they explore how they can exploit the different the parallel nature of CNNs. They introduce types of parallelism found in CNNs, \textit{inter-output} and \textit{intra-output}. The first one comes from the observation that each feature map and the corresponding subsampling/pooling computation can be done in parallel. This is easily seen in the first layer. The second one refers to that the convolution of several inputs are combined to produce one feature map (see Figure \ref{fig_visual_conv_ss_mp}), where the individual convolutions can be done in parallel. This one is present in all of the convolution layers after the first layer. They exploit these observations by purposing a dynamically configurable coprocessor on a FPGA, which can switch between computing several different feature maps in parallel and processing several inputs to compute one feature map. By doing this they are able to fully utilize the parallel nature of a CNN and reduce the intermediate storage on the FPGA. Using a Virtex 5 SX240T FPGA with 1024 multiply-accumulate units they were able to outperform  a 2.3 GHz quad-core, dual socket Intel Xeon, and a 1.35 GHz C870 GPU by 4x to 8x. 

\cite{Paper} presents an architecture they named the \textit{nn-X}. For the implementation they used a Xilinx ZC706 platform, containing a Kintex-7 FPGA and two ARM Cortex-A9. They made acceleration units for the convolution and subsampling/pooling layers on the FPGA, while the fully-connected layers was processed by the ARM processors. [REVWRITE]This implementation seems to be the fastest of all the purposed FPGA implementations to date, able to perform up to 227 G-ops/s. In addition it seems to be the most energy efficient across all platforms, by being able to perform 25 G-ops/s-Watt. In the paper they compared it to a Intel i7-3720QM and a NVIDIA’s GTX 780 GPU, where it was shown that it was up to 20x more power efficient. This makes a strong case for hardware acceleration of CNNs in applications designed for energy efficiency. 

\cite{INSERT_PAPER} focuses one the challenges of memory bandwidth related to deep convolutional neural networks. They argue that while accelerators are fast, slow memory makes it difficult to saturate the accelerators with enough data. To combat this they purpose a memory access optimized routcing scheme, where they try do reduce the number of times a input map has to be transfered from memory to the accelerator. A crucial point here is that in general the output map is the sum of several convoluted input maps. Thus if a accelerator is only able to compute one output map at a time, the input maps have to be transfered to the accelerator several times. This architecture suggested reduces the amount of such transfers by having a DMA for every two accelerator, and making the DMA transfer the same data to both of its accelerators. The accelerators will either produce an intermediate results or a complete output map, depending on how many iterations it has run. There is a total if eight accelerators, four which used to combine intermediate results into complete output maps, and four to compute intermediate results. Using this memeory scheme they were able to decrease the memory access by 2x and increase the hardware utilization by 2x. 

Hardware acceleration of CNNs have also gained popularity within the data center field. The Internet and Big Data have made it viable to have servers that performs image classification, image recognition and natural language processing, using CNNs. Since the main expense of data centers are power usage, using specialized hardware accelerators that provides good performance at low power have gained increased popularity. A recent example is Microsoft which have experimented on using accelerators for CNNs in their data centers, which is described in \cite{Microsoft_paper}. In the paper they present a rather big architecture compared to the previous papers mentioned, which fit into a Stratix V D5 FPGA. The previous mentioned architectures generally contained accelerators that computed parts of the network and needed to be controlled by software through macro instructions. Something that can cause a lot of off-chip traffic. In contrast, this architecture is able to reduce software involvement and off-chip traffic by using an on-chip re-distribution network module, which reloads processed data back into the accelerator. The cost of reducing off-chip traffic this way causes the architecture to consume a larger amount of hardware resources, which is the reason they are using such a big FPGA as the Stratix V D5. It would thus not be viable for mobile application with an restricted amount of hardware resources, but suits servers well. The result is a 3x improvement from the previous best implementation of a Virtex 7, 134 images/s compared to 46 images/s. The system is still slower than a GPU implementation on a Tesla K40, which are able to process 500-824 images/s, but is almost twice as energy efficient. The FPGA implementation uses 25 W, while the Tesla K40 runs at 235 W. 

